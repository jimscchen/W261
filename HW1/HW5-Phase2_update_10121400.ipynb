{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS - w261 Machine Learning At Scale\n",
    "__Course Lead:__ Dr James G. Shanahan (__email__ Jimi via  James.Shanahan _AT_ gmail.com)\n",
    "\n",
    "## Assignment - HW5 - Phase 2\n",
    "\n",
    "\n",
    "---\n",
    "__Group Members:__  \n",
    "*Jim Chen, Memphis, TN, jim.chen@ischool.berkeley.edu*  \n",
    "*Manuel Moreno, Salt Lake City, UT, momoreno@ischool.berkeley.edu*  \n",
    "*Rahul Ragunathan, Lansing, MI, rahulragunathan@ischool.berkeley.edu*  \n",
    "*Jason Sanchez, San Francisco, CA, jason.sanchez@ischool.berkeley.edu*  \n",
    "__Class:__ MIDS w261 Fall 2016 Group 2  \n",
    "__Week:__   5\n",
    "\n",
    "__Due Time:__ 2 Phases. \n",
    "\n",
    "* __HW5 Phase 1__ \n",
    "This can be done on a local machine (with a unit test on the cloud such as AltaScale's PaaS or on AWS) and is due Tuesday, Week 6 by 8AM (West coast time). It will primarily focus on building a unit/systems and for pairwise similarity calculations pipeline (for stripe documents)\n",
    "\n",
    "* __HW5 Phase 2__ \n",
    "This will require the AltaScale cluster and will be due Tuesday, Week 7 by 8AM (West coast time). \n",
    "The focus of  HW5 Phase 2  will be to scale up the unit/systems tests to the Google 5 gram corpus. This will be a group exercise \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\">\n",
    "# Instructions </a>\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #5\n",
    "\n",
    "Version 2016-09-25 \n",
    "\n",
    " === INSTRUCTIONS for SUBMISSIONS ===\n",
    "Follow the instructions for submissions carefully.\n",
    "\n",
    "https://docs.google.com/forms/d/1ZOr9RnIe_A06AcZDB6K1mJN4vrLeSmS2PD6Xm3eOiis/viewform?usp=send_form \n",
    "\n",
    "\n",
    "### Documents:\n",
    "* IPython Notebook, published and viewable online.\n",
    "* PDF export of IPython Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a name=\"TOC\"></a> \n",
    "\n",
    "* [HW5.0](#1.0): Short answers  \n",
    "* [HW5.1](#1.1): Short answers  \n",
    "* [HW5.2](#1.2): Joins  \n",
    "* [Phase 1](#1.3): (Local) System Test  \n",
    "    * [HW5.3](#1.3)   \n",
    "* [Phase 2](#2.0)  \n",
    "    * [Vocab Identification](#2.1)  \n",
    "    * [Stripe Creation](#2.2)  \n",
    "    * [InvertIndex Creation](#2.3)  \n",
    "    * [Similarity Calculation](#2.4)   \n",
    "    * [Ngram Ranking and Plotting](#2.5)  \n",
    "    * [NLTK Benchmarking](#2.6)   \n",
    "[HW5.4](#1.4)    \n",
    "[HW5.5](#1.5)    \n",
    "[HW5.6](#1.6)    \n",
    "[HW5.7](#1.7)    \n",
    "[HW5.8](#1.8)    \n",
    "[HW5.9](#1.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\">\n",
    "# HW Problems\n",
    "[Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.0  <a name=\"1.0\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "- What is a data warehouse? What is a Star schema? When is it used?\n",
    "\n",
    "Data warehouse: Stores a large amount of relational, semi-structured, and unstructured data. Is used for business intelligence and data science.\n",
    "\n",
    "A star schema has fact tables and many dimension tables that connect to the fact tables. Fact tables record events such as sales or website visits and encodes details of the events as keys (user_id, product_id, store_id, ad_id). The dimension tables store the detailed information about each of these keys. \n",
    "\n",
    "Star schemas provide simple approached to structuring data warehouses in a relational way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.1  <a name=\"1.1\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "- In the database world What is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "- In what form does ML consume data?\n",
    "- Why would one use log files that are denormalized?\n",
    "\n",
    "3NF means third normal form. It is used to transform large flat files that have repeated data into a linked collection of smaller tables that can be joined on a set of common keys.\n",
    "\n",
    "Machine learning does not use data in 3NF. Instead it uses large flat files so the details that are hidden by the keys can be used in the algorithms.\n",
    "\n",
    "Log files can track specific events of interest. A denormalized log file allows a company to track these events in real time conditioned on specific customer features. Alternatively, a model can be running that triggers appropriate responses based on the next predicted action of a user given the user's latest action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.2  <a name=\"1.2\"></a>\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List data files to use for joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anonymous-msweb-preprocessed.data\r\n",
      "mostFrequentVisitors.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls | grep \"mo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Count lines in log dataset. View the first 10 lines. Rename data to log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   98654 anonymous-msweb-preprocessed.data\n",
      "\n",
      "V,1000,1,C,10001\n",
      "V,1001,1,C,10001\n",
      "V,1002,1,C,10001\n",
      "V,1001,1,C,10002\n",
      "V,1003,1,C,10002\n",
      "V,1001,1,C,10003\n",
      "V,1003,1,C,10003\n",
      "V,1004,1,C,10003\n",
      "V,1005,1,C,10004\n",
      "V,1006,1,C,10005\n"
     ]
    }
   ],
   "source": [
    "!wc -l anonymous-msweb-preprocessed.data && echo\n",
    "!head anonymous-msweb-preprocessed.data\n",
    "!cp anonymous-msweb-preprocessed.data log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Convert the output of 4.4 to be just url and url_id. Save as urls.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     285 urls.txt\n",
      "\n",
      "\"/regwiz\",1000\n",
      "\"/support\",1001\n",
      "\"/athome\",1002\n",
      "\"/kb\",1003\n",
      "\"/search\",1004\n",
      "\"/norge\",1005\n",
      "\"/misc\",1006\n",
      "\"/ie_intl\",1007\n",
      "\"/msdownload\",1008\n",
      "\"/windows\",1009\n"
     ]
    }
   ],
   "source": [
    "!cat mostFrequentVisitors.txt | cut -f 1,2 -d',' > urls.txt\n",
    "!wc -l urls.txt && echo\n",
    "!head urls.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The urls.txt file is much smaller than the log.txt data and should be what is loaded into memory. This means it would be the right-side table in a left-side join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting join.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile join.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# Avoid broken pipe error\n",
    "from signal import signal, SIGPIPE, SIG_DFL\n",
    "signal(SIGPIPE,SIG_DFL) \n",
    "\n",
    "class Join(MRJob):\n",
    "    def configure_options(self):\n",
    "        super(Join, self).configure_options()\n",
    "        \n",
    "        self.add_passthrough_option(\n",
    "            '--join', \n",
    "            default=\"left\", \n",
    "            help=\"Options: left, inner, right\")\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        self.join = self.options.join\n",
    "        self.urls_used = set()\n",
    "        self.urls = {}\n",
    "        \n",
    "        try:\n",
    "            open(\"urls.txt\")\n",
    "            filename = \"urls.txt\"\n",
    "        except FileNotFoundError:\n",
    "            filename = \"limited_urls.txt\"\n",
    "        \n",
    "        with open(filename) as urls:\n",
    "            for line in urls:\n",
    "                url, key = line.strip().replace('\"',\"\").split(\",\")\n",
    "                self.urls[key] = url\n",
    "        \n",
    "    def mapper(self, _, lines):\n",
    "        try:\n",
    "            url = lines[2:6]\n",
    "            if self.join in [\"inner\", \"left\"]:\n",
    "                yield (lines, self.urls[url])\n",
    "            elif self.join in [\"right\"]:\n",
    "                yield (self.urls[url], lines)\n",
    "                self.urls_used.add(url)\n",
    "            \n",
    "        except KeyError:\n",
    "            if self.join in [\"inner\", \"right\"]:\n",
    "                pass\n",
    "            else:\n",
    "                yield (lines, \"None\")\n",
    "\n",
    "    def mapper_final(self):\n",
    "        for key, value in self.urls.items():\n",
    "            if key not in self.urls_used:\n",
    "                yield (self.urls[key], \"*\")\n",
    "                \n",
    "    def reducer(self, url, values):\n",
    "        quick_stash = 0\n",
    "        for val in values:\n",
    "            if val != \"*\":\n",
    "                quick_stash += 1\n",
    "                yield (val, url)\n",
    "        if quick_stash == 0:\n",
    "            yield (\"None\", url)\n",
    "            \n",
    "    def steps(self):\n",
    "        join = self.options.join\n",
    "        if join in [\"inner\", \"left\"]:\n",
    "            mrsteps = [MRStep(mapper_init=self.mapper_init,\n",
    "                              mapper=self.mapper)]\n",
    "        if join == \"right\":\n",
    "            mrsteps = [MRStep(mapper_init=self.mapper_init,\n",
    "                              mapper=self.mapper,\n",
    "                              mapper_final=self.mapper_final,\n",
    "                              reducer=self.reducer)]  \n",
    "        return mrsteps\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    Join.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a file with only the first five urls to test left and inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head -n 5 urls.txt > limited_urls.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the first ten lines of the log file and left joining it to the first five lines of the urls file, we see that some of the urls are returned as \"None.\" This is correct behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"V,1000,1,C,10001\"\t\"/regwiz\"\r\n",
      "\"V,1001,1,C,10001\"\t\"/support\"\r\n",
      "\"V,1002,1,C,10001\"\t\"/athome\"\r\n",
      "\"V,1001,1,C,10002\"\t\"/support\"\r\n",
      "\"V,1003,1,C,10002\"\t\"/kb\"\r\n",
      "\"V,1001,1,C,10003\"\t\"/support\"\r\n",
      "\"V,1003,1,C,10003\"\t\"/kb\"\r\n",
      "\"V,1004,1,C,10003\"\t\"/search\"\r\n",
      "\"V,1005,1,C,10004\"\t\"None\"\r\n",
      "\"V,1006,1,C,10005\"\t\"None\"\r\n"
     ]
    }
   ],
   "source": [
    "!head log.txt | python join.py --file limited_urls.txt --join left -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the same operation, but with an inner join, we see the lines that were \"None\" are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"V,1000,1,C,10001\"\t\"/regwiz\"\r\n",
      "\"V,1001,1,C,10001\"\t\"/support\"\r\n",
      "\"V,1002,1,C,10001\"\t\"/athome\"\r\n",
      "\"V,1001,1,C,10002\"\t\"/support\"\r\n",
      "\"V,1003,1,C,10002\"\t\"/kb\"\r\n",
      "\"V,1001,1,C,10003\"\t\"/support\"\r\n",
      "\"V,1003,1,C,10003\"\t\"/kb\"\r\n",
      "\"V,1004,1,C,10003\"\t\"/search\"\r\n"
     ]
    }
   ],
   "source": [
    "!head log.txt | python join.py --file limited_urls.txt --join inner -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove the right-side join works, we can only use the first 100 log entries. We see that urls without corresponding log entries are listed as \"None\" and that all urls are returned in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"None\"\t\"/access\"\r\n",
      "\"None\"\t\"/accessdev\"\r\n",
      "\"None\"\t\"/activeplatform\"\r\n",
      "\"None\"\t\"/activex\"\r\n",
      "\"None\"\t\"/adc\"\r\n",
      "\"None\"\t\"/ado\"\r\n",
      "\"None\"\t\"/ads\"\r\n",
      "\"None\"\t\"/advtech\"\r\n",
      "\"None\"\t\"/argentina\"\r\n",
      "\"None\"\t\"/atec\"\r\n",
      "\"V,1002,1,C,10001\"\t\"/athome\"\r\n",
      "\"V,1002,1,C,10019\"\t\"/athome\"\r\n",
      "\"V,1002,1,C,10020\"\t\"/athome\"\r\n",
      "\"V,1002,1,C,10031\"\t\"/athome\"\r\n",
      "\"None\"\t\"/australia\"\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 100 log.txt | python join.py --file urls.txt --join right -r local -q | head -n 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the limited urls file, we see that only five urls are returned and every logged page visit to those pages are returned (at least within the first 50 log entries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"V,1002,1,C,10001\"\t\"/athome\"\r\n",
      "\"V,1002,1,C,10019\"\t\"/athome\"\r\n",
      "\"V,1003,1,C,10002\"\t\"/kb\"\r\n",
      "\"V,1003,1,C,10003\"\t\"/kb\"\r\n",
      "\"V,1003,1,C,10006\"\t\"/kb\"\r\n",
      "\"V,1003,1,C,10019\"\t\"/kb\"\r\n",
      "\"V,1000,1,C,10001\"\t\"/regwiz\"\r\n",
      "\"V,1000,1,C,10010\"\t\"/regwiz\"\r\n",
      "\"V,1004,1,C,10003\"\t\"/search\"\r\n",
      "\"V,1004,1,C,10006\"\t\"/search\"\r\n",
      "\"V,1004,1,C,10008\"\t\"/search\"\r\n",
      "\"V,1004,1,C,10018\"\t\"/search\"\r\n",
      "\"V,1004,1,C,10019\"\t\"/search\"\r\n",
      "\"V,1001,1,C,10001\"\t\"/support\"\r\n",
      "\"V,1001,1,C,10002\"\t\"/support\"\r\n",
      "\"V,1001,1,C,10003\"\t\"/support\"\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 50 log.txt | python join.py --file limited_urls.txt --join right -r local -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5.3 <a name=\"1.3\"></a> Systems tests on n-grams dataset (Phase1) and full experiment (Phase 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "## 3.  HW5.3.0 Run Systems tests locally (PHASE1)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox and on s3:\n",
    "\n",
    "https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "The next cell shows the first 10 lines of the googlebooks-eng-all-5gram-20090715-0-filtered.txt file.\n",
    "\n",
    "\n",
    "__DISCLAIMER__: Each record is already a 5-gram. We should calculate the stripes cooccurrence data from the raw text and not from the 5-gram preprocessed data. Calculating pairs on this 5-gram is a little corrupt as we will be double counting cooccurences. Having said that this exercise can still pull out some similar terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for systems test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mini_5gram.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mini_5gram.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile mini_5gram.txt\n",
    "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\n",
    "A Biography of General George\t92\t90\t74\n",
    "A Case Study in Government\t102\t102\t78\n",
    "A Case Study of Female\t447\t447\t327\n",
    "A Case Study of Limited\t55\t55\t43\n",
    "A Child's Christmas in Wales\t1099\t1061\t866\n",
    "A Circumstantial Narrative of the\t62\t62\t50\n",
    "A City by the Sea\t62\t60\t49\n",
    "A Collection of Fairy Tales\t123\t117\t80\n",
    "A Collection of Forms of\t116\t103\t82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "atlas.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting atlas.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas.txt\n",
    "atlas boon\t50\t50\t50\n",
    "boon cava dipped\t10\t10\t10\n",
    "atlas dipped\t15\t15\t15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mini_stripes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"DocA\"\t{\"X\":20, \"Y\":30, \"Z\":5}\r\n",
      "\"DocB\"\t{\"X\":100, \"Y\":20}\r\n",
      "\"DocC\"\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\r\n"
     ]
    }
   ],
   "source": [
    "with open(\"mini_stripes.txt\", \"w\") as f:\n",
    "    f.writelines([\n",
    "        '\"DocA\"\\t{\"X\":20, \"Y\":30, \"Z\":5}\\n',\n",
    "        '\"DocB\"\\t{\"X\":100, \"Y\":20}\\n',  \n",
    "        '\"DocC\"\\t{\"M\":5, \"N\":20, \"Z\":5, \"Y\":1}\\n'\n",
    "    ])\n",
    "!cat mini_stripes.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK: Phase 1\n",
    "Complete 5.4 and 5.5 and systems test them using the above test datasets. Phase 2 will focus on the entire Ngram dataset.\n",
    "\n",
    "To help you through these tasks please verify that your code gives the following results (for stripes, inverted index, and pairwise similarities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MakeStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MakeStripes.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from collections import Counter\n",
    "\n",
    "class MakeStripes(MRJob):\n",
    "    def mapper(self, _, lines):\n",
    "        terms, term_count, page_count, book_count = lines.split(\"\\t\")\n",
    "        terms = terms.split()\n",
    "        term_count = int(term_count)\n",
    "        \n",
    "        for item in terms:\n",
    "            yield (item, {term:term_count for term in terms if term != item})\n",
    "        \n",
    "    def combiner(self, keys, values):\n",
    "        values_sum = Counter()\n",
    "        for val in values:\n",
    "            values_sum += Counter(val)\n",
    "        yield keys, dict(values_sum)\n",
    "\n",
    "    def reducer(self, keys, values):\n",
    "        values_sum = Counter()\n",
    "        for val in values:\n",
    "            values_sum += Counter(val)\n",
    "        yield keys, dict(values_sum)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MakeStripes.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desired result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing atlas_desired_results.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile atlas_desired_results.txt\n",
    "\n",
    "\"atlas\"\t{\"dipped\": 15, \"boon\": 50}\n",
    "\"boon\"\t{\"atlas\": 50, \"dipped\": 10, \"cava\": 10}\n",
    "\"cava\"\t{\"dipped\": 10, \"boon\": 10}\n",
    "\"dipped\"\t{\"atlas\": 15, \"boon\": 10, \"cava\": 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"dipped\": 15, \"boon\": 50}\r\n",
      "\"boon\"\t{\"atlas\": 50, \"dipped\": 10, \"cava\": 10}\r\n",
      "\"cava\"\t{\"dipped\": 10, \"boon\": 10}\r\n",
      "\"dipped\"\t{\"atlas\": 15, \"boon\": 10, \"cava\": 10}\r\n"
     ]
    }
   ],
   "source": [
    "!cat atlas-boon-systems-test.txt | python MakeStripes.py -q > atlas_stripes.txt\n",
    "!cat atlas_stripes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual result matches desired result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting InvertIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile InvertIndex.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONProtocol\n",
    "from collections import Counter\n",
    "\n",
    "class InvertIndex(MRJob):\n",
    "    MRJob.input_protocol = JSONProtocol\n",
    "    \n",
    "    def mapper(self, key, words):\n",
    "        n_words = len(words)\n",
    "        \n",
    "        for word in words: \n",
    "            yield (word, {key:n_words})\n",
    "            \n",
    "    def combiner(self, keys, values):\n",
    "        values_sum = Counter()\n",
    "        for val in values:\n",
    "            values_sum += Counter(val)\n",
    "        yield keys, dict(values_sum)\n",
    "\n",
    "    def reducer(self, keys, values):\n",
    "        values_sum = Counter()\n",
    "        for val in values:\n",
    "            values_sum += Counter(val)\n",
    "        yield keys, dict(values_sum)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    InvertIndex.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desired result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systems test mini_stripes - Inverted Index   \n",
    "——————————————————————————————————————————   \n",
    "            \"M\" |         DocC 4 |          \n",
    "            \"N\" |         DocC 4 |           \n",
    "            \"X\" |         DocA 3 |         DocB 2 |               \n",
    "            \"Y\" |         DocA 3 |         DocB 2 |         DocC 4   |   \n",
    "            \"Z\" |         DocA 3 |         DocC 4 |               \n",
    "\n",
    " systems test atlas-boon - Inverted Index   \n",
    "——————————————————————————————————————————   \n",
    "        \"atlas\" |         boon 3 |       dipped 3 |               \n",
    "       \"dipped\" |        atlas 2 |         boon 3 |         cava 2  |  \n",
    "         \"boon\" |        atlas 2 |         cava 2 |       dipped 3  |  \n",
    "         \"cava\" |         boon 3 |       dipped 3 |        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"M\"\t{\"DocC\": 4}\r\n",
      "\"N\"\t{\"DocC\": 4}\r\n",
      "\"X\"\t{\"DocA\": 3, \"DocB\": 2}\r\n",
      "\"Y\"\t{\"DocC\": 4, \"DocA\": 3, \"DocB\": 2}\r\n",
      "\"Z\"\t{\"DocC\": 4, \"DocA\": 3}\r\n"
     ]
    }
   ],
   "source": [
    "!cat mini_stripes.txt | python InvertIndex.py -q > mini_stripes_inverted.txt\n",
    "!cat mini_stripes_inverted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"atlas\"\t{\"dipped\": 3, \"boon\": 3}\r\n",
      "\"boon\"\t{\"dipped\": 3, \"atlas\": 2, \"cava\": 2}\r\n",
      "\"cava\"\t{\"dipped\": 3, \"boon\": 3}\r\n",
      "\"dipped\"\t{\"atlas\": 2, \"boon\": 3, \"cava\": 2}\r\n"
     ]
    }
   ],
   "source": [
    "!cat atlas_stripes.txt | python InvertIndex.py -q > atlas_inverted.txt\n",
    "!cat atlas_inverted.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tests pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Similarity.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONProtocol\n",
    "from itertools import combinations\n",
    "from statistics import mean\n",
    "\n",
    "class Similarity(MRJob):\n",
    "    MRJob.input_protocol = JSONProtocol\n",
    "    \n",
    "    def mapper(self, key_term, docs):\n",
    "        doc_names = docs.keys()\n",
    "        for doc_pairs in combinations(sorted(list(doc_names)), 2):\n",
    "            yield (doc_pairs, 1)\n",
    "        for name in doc_names:\n",
    "            yield (name, 1)\n",
    "            \n",
    "    def combiner(self, key, value):\n",
    "        yield (key, sum(value))\n",
    "        \n",
    "    def reducer_init(self):\n",
    "        self.words = {}\n",
    "        self.results = []\n",
    "    \n",
    "    def reducer(self, doc_or_docs, count):\n",
    "        if isinstance(doc_or_docs, str):\n",
    "            self.words[doc_or_docs] = sum(count)\n",
    "        else:\n",
    "            d1, d2 = doc_or_docs\n",
    "            d1_n_words, d2_n_words = self.words[d1], self.words[d2]\n",
    "            intersection = sum(count)\n",
    "            \n",
    "            jaccard = round(intersection/(d1_n_words + d2_n_words - intersection), 3)\n",
    "            cosine = round(intersection/(d1_n_words**.5 * d2_n_words**.5), 3)\n",
    "            dice = round(2*intersection/(d1_n_words + d2_n_words), 3)\n",
    "            overlap = round(intersection/min(d1_n_words, d2_n_words), 3)\n",
    "            average = round(mean([jaccard, cosine, dice, overlap]), 3)\n",
    "            \n",
    "            self.results.append([doc_or_docs, {\"jacc\":jaccard, \"cos\":cosine, \n",
    "                                               \"dice\":dice, \"ol\":overlap, \"ave\":average}])\n",
    "            \n",
    "    def reducer_final(self):\n",
    "        for doc, result in sorted(self.results, key=lambda x: x[1][\"ave\"], reverse=True):\n",
    "            yield (doc, result)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    Similarity.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desired results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systems test mini_stripes - Similarity measures\n",
    "\n",
    "| average |           pair |         cosine |        jaccard |        overlap |           dice |\n",
    "|-|-|-|-|-|-|\n",
    "|       0.741582 |    DocA - DocB |       0.816497 |       0.666667 |       1.000000 |       0.800000 |\n",
    "|       0.488675 |    DocA - DocC |       0.577350 |       0.400000 |       0.666667 |       0.571429 |\n",
    "|       0.276777 |    DocB - DocC |       0.353553 |       0.200000 |       0.500000 |       0.333333 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Systems test atlas-boon 2 - Similarity measures\n",
    "\n",
    "| average |           pair |         cosine |        jaccard |        overlap |           dice |\n",
    "|-|-|-|-|-|-|       \n",
    "|1.000000 |   atlas - cava |       1.000000 |       1.000000 |        1.000000 |       1.000000|\n",
    "|       0.625000 |  boon - dipped |       0.666667 |       0.500000 |       0.666667 |       0.666667|\n",
    "|       0.389562 |  cava - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000|\n",
    "|       0.389562 |    boon - cava |       0.408248 |       0.250000 |       0.500000 |       0.400000|\n",
    "|       0.389562 | atlas - dipped |       0.408248 |       0.250000 |       0.500000 |       0.400000|\n",
    "|       0.389562 |   atlas - boon |       0.408248 |       0.250000 |       0.500000 |       0.400000|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"DocA\", \"DocB\"]\t{\"cos\": 0.816, \"ave\": 0.821, \"jacc\": 0.667, \"dice\": 0.8, \"ol\": 1.0}\r\n",
      "[\"DocA\", \"DocC\"]\t{\"cos\": 0.577, \"ave\": 0.554, \"jacc\": 0.4, \"dice\": 0.571, \"ol\": 0.667}\r\n",
      "[\"DocB\", \"DocC\"]\t{\"cos\": 0.354, \"ave\": 0.347, \"jacc\": 0.2, \"dice\": 0.333, \"ol\": 0.5}\r\n"
     ]
    }
   ],
   "source": [
    "!cat mini_stripes_inverted.txt | python Similarity.py -q --jobconf mapred.reduce.tasks=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"atlas\", \"cava\"]\t{\"ol\": 1.0, \"jacc\": 1.0, \"cos\": 1.0, \"ave\": 1.0, \"dice\": 1.0}\r\n",
      "[\"boon\", \"dipped\"]\t{\"ol\": 0.667, \"jacc\": 0.5, \"cos\": 0.667, \"ave\": 0.625, \"dice\": 0.667}\r\n",
      "[\"atlas\", \"boon\"]\t{\"ol\": 0.5, \"jacc\": 0.25, \"cos\": 0.408, \"ave\": 0.39, \"dice\": 0.4}\r\n",
      "[\"atlas\", \"dipped\"]\t{\"ol\": 0.5, \"jacc\": 0.25, \"cos\": 0.408, \"ave\": 0.39, \"dice\": 0.4}\r\n",
      "[\"boon\", \"cava\"]\t{\"ol\": 0.5, \"jacc\": 0.25, \"cos\": 0.408, \"ave\": 0.39, \"dice\": 0.4}\r\n",
      "[\"cava\", \"dipped\"]\t{\"ol\": 0.5, \"jacc\": 0.25, \"cos\": 0.408, \"ave\": 0.39, \"dice\": 0.4}\r\n"
     ]
    }
   ],
   "source": [
    "!cat atlas_inverted.txt | python Similarity.py -q --jobconf mapred.reduce.tasks=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers calculated exactly match the systems test except for the average calculations of the mini_stripes set. In this instance, the systems test calculations are not correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From beginning to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"atlas\", \"cava\"]\t{\"ave\": 1.0, \"jacc\": 1.0, \"ol\": 1.0, \"dice\": 1.0, \"cos\": 1.0}\r\n",
      "[\"boon\", \"dipped\"]\t{\"ave\": 0.625, \"jacc\": 0.5, \"ol\": 0.667, \"dice\": 0.667, \"cos\": 0.667}\r\n",
      "[\"atlas\", \"boon\"]\t{\"ave\": 0.39, \"jacc\": 0.25, \"ol\": 0.5, \"dice\": 0.4, \"cos\": 0.408}\r\n",
      "[\"atlas\", \"dipped\"]\t{\"ave\": 0.39, \"jacc\": 0.25, \"ol\": 0.5, \"dice\": 0.4, \"cos\": 0.408}\r\n",
      "[\"boon\", \"cava\"]\t{\"ave\": 0.39, \"jacc\": 0.25, \"ol\": 0.5, \"dice\": 0.4, \"cos\": 0.408}\r\n",
      "[\"cava\", \"dipped\"]\t{\"ave\": 0.39, \"jacc\": 0.25, \"ol\": 0.5, \"dice\": 0.4, \"cos\": 0.408}\r\n"
     ]
    }
   ],
   "source": [
    "!cat atlas-boon-systems-test.txt | python MakeStripes.py -q | python InvertIndex.py -q | python Similarity.py -q --jobconf mapred.reduce.tasks=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.0\">\n",
    "# PHASE 2: Full-scale experiment on Google N-gram data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.1\">\n",
    "## 2.1 Vocab Identification\n",
    "[Back to Table of Contents](#TOC)  \n",
    "\n",
    "This section scans through the corpus file(s) and identify the top-n frequent words as vocabularies.\n",
    "\n",
    "We utilize heapq to reduce the amount of data to transfer using hadoop. \n",
    "\n",
    "This approach can run into memory constraints if our goal is to return the top k results where k is so large the resulting ordered list cannot fit into memory on a single machine (i.e. billions of results). In practice, we only care about a small number of the top results (for example, in this problem we only need to return the top 1000 results. 1000 results are trivially stored in memory).\n",
    "\n",
    "The code uses multiple reducers. In the last MapReduce step, all data is sent to a single reducer by use of a single key; however, the data that is sent is never stored in memory (only the top k results are) and at most k*n_reducers observations would be sent to this reducer, which means the total data sent is very small and could easily fit on a single hard drive. If the data is so large it cannot fit on a single hard drive, we could add more MR steps to reduce the size of the data by 90% for each added step. \n",
    "\n",
    "That said, we estimate that the code could work without any changes on a dataset with 100 trillion words if we were asked to return the top 100,000 words and had a cluster of 1,000 machines available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting GetIndexandOtherWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile GetIndexandOtherWords.py\n",
    "\n",
    "import heapq\n",
    "from re import findall\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "\n",
    "class TopList(list):\n",
    "    def __init__(self, max_size, num_position=0):\n",
    "        \"\"\"\n",
    "        Just like a list, except the append method adds the new value to the \n",
    "        list only if it is larger than the smallest value (or if the size of \n",
    "        the list is less than max_size). \n",
    "        \n",
    "        If each element of the list is an int or float, uses that value for \n",
    "        comparison. If the elements in the list are lists or tuples, uses the \n",
    "        list_position element of the list or tuple for the comparison.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.pos = num_position\n",
    "        \n",
    "    def _get_key(self, x):\n",
    "        return x[self.pos] if isinstance(x, (list, tuple)) else x\n",
    "        \n",
    "    def append(self, val):\n",
    "        if len(self) < self.max_size:\n",
    "            heapq.heappush(self, val)\n",
    "        elif self._get_key(self[0]) < self._get_key(val):\n",
    "            heapq.heapreplace(self, val)\n",
    "            \n",
    "    def final_sort(self):\n",
    "        return sorted(self, key=self._get_key, reverse=True)\n",
    "\n",
    "    \n",
    "class GetIndexandOtherWords(MRJob):\n",
    "    \"\"\"\n",
    "    Usage: python GetIndexandOtherWords.py --index-range 9000-10000 --top-n-words 10000 --use-term-counts True\n",
    "    \n",
    "    Given n-gram formatted data, outputs a file of the form:\n",
    "    \n",
    "    index    term\n",
    "    index    term\n",
    "    ...\n",
    "    word     term\n",
    "    word     term\n",
    "    ...\n",
    "    \n",
    "    Where there would be 1001 index words and 10000 total words. Each word would be ranked based\n",
    "    on either the term count listed in the Google n-gram data (i.e. the counts found in the\n",
    "    underlying books) or the ranks would be based on the word count of the n-grams in the actual\n",
    "    dataset (i.e. ignore the numbers/counts associated with each n-gram and count each n-gram\n",
    "    exactly once).\n",
    "    \"\"\"\n",
    "    def configure_options(self):\n",
    "        super(GetIndexandOtherWords, self).configure_options()\n",
    "        \n",
    "        self.add_passthrough_option(\n",
    "            '--index-range', \n",
    "            default=\"9-10\", \n",
    "            help=\"Specify the range of the index words. ex. 9-10 means the ninth and \" +\n",
    "                 \"tenth most popular words will serve as the index\")\n",
    "        \n",
    "        self.add_passthrough_option(\n",
    "            '--top-n-words', \n",
    "            default=\"10\", \n",
    "            help=\"Specify the number of words to output in all\")\n",
    "        \n",
    "        self.add_passthrough_option(\n",
    "            '--use-term-counts', \n",
    "            default=\"True\", \n",
    "            choices=[\"True\",\"False\"],\n",
    "            help=\"When calculating the most frequent words, choose whether to count \" + \n",
    "            \"each word based on the term counts reported by Google or just based on \" + \n",
    "            \"the number of times the word appears in an n-gram\")\n",
    "        \n",
    "        self.add_passthrough_option(\n",
    "            '--return-counts', \n",
    "            default=\"False\", \n",
    "            choices=[\"True\",\"False\"],\n",
    "            help=\"The final output includes the counts of each word\")\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        # Ensure command line options are sane\n",
    "        top_n_words = int(self.options.top_n_words)\n",
    "        last_index_word = int(self.options.index_range.split(\"-\")[1])\n",
    "        if top_n_words < last_index_word:\n",
    "            raise ValueError(\"\"\"--top-n-words value (currently %d) must be equal to or greater than\n",
    "                             --index-range value (currently %d).\"\"\" % (top_n_words, last_index_word))\n",
    "        \n",
    "        self.stop_words =  set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', \n",
    "                            'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', \n",
    "                            'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', \n",
    "                            'its', 'itself', 'they', 'them', 'their', 'theirs', \n",
    "                            'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "                            'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n",
    "                            'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "                            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', \n",
    "                            'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "                            'with', 'about', 'against', 'between', 'into', 'through', \n",
    "                            'during', 'before', 'after', 'above', 'below', 'to', 'from', \n",
    "                            'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', \n",
    "                            'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
    "                            'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
    "                            'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n",
    "                            'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', \n",
    "                            'can', 'will', 'just', 'don', 'should', 'now'])\n",
    "        \n",
    "    def mapper(self, _, lines):\n",
    "        terms, term_count, page_count, book_count = lines.split(\"\\t\")\n",
    "        \n",
    "        # Either use the ngram term count for the count or count each word just once\n",
    "        if self.options.use_term_counts == \"True\":\n",
    "            term_count = int(term_count)\n",
    "        else:\n",
    "            term_count = 1\n",
    "        \n",
    "        # Iterate through each term. Skip stop words\n",
    "        for term in findall(r'[a-z]+', terms.lower()):\n",
    "            if term in self.stop_words:\n",
    "                pass\n",
    "            else:\n",
    "                yield (term, term_count)\n",
    "        \n",
    "    def combiner(self, term, counts):\n",
    "        yield (term, sum(counts))      \n",
    "        \n",
    "        \n",
    "    def reducer_init(self):\n",
    "        \"\"\"\n",
    "        Accumulates the top X words and yields them. Note: should only use if\n",
    "        you want to emit a reasonable amount of top words (i.e. an amount that \n",
    "        could fit on a single computer.)\n",
    "        \"\"\"\n",
    "        self.top_n_words = int(self.options.top_n_words)\n",
    "        self.TopTerms = TopList(self.top_n_words, num_position=1)\n",
    "        \n",
    "    def reducer(self, term, counts):\n",
    "        self.TopTerms.append((term, sum(counts)))\n",
    "\n",
    "    def reducer_final(self):\n",
    "        for pair in self.TopTerms:\n",
    "            yield pair\n",
    "        \n",
    "    def mapper_single_key(self, term, count):\n",
    "        \"\"\"\n",
    "        Send all the data to a single reducer\n",
    "        \"\"\"\n",
    "        yield (1, (term, count))\n",
    "        \n",
    "    def reducer_init_top_vals(self):\n",
    "        # Collect top words\n",
    "        self.top_n_words = int(self.options.top_n_words)\n",
    "        self.TopTerms = TopList(self.top_n_words, num_position=1)\n",
    "        # Collect index words\n",
    "        self.index_range = [int(num) for num in self.options.index_range.split(\"-\")]\n",
    "        self.index_low, self.index_high = self.index_range\n",
    "        # Control if output shows counts or just words\n",
    "        self.return_counts = self.options.return_counts == \"True\"\n",
    "\n",
    "    def reducer_top_vals(self, _, terms):\n",
    "        for term in terms:\n",
    "            self.TopTerms.append(term)\n",
    "            \n",
    "    def reducer_final_top_vals(self):\n",
    "        TopTerms = self.TopTerms.final_sort()\n",
    "        \n",
    "        if self.return_counts:            \n",
    "            # Yield index words\n",
    "            for term in TopTerms[self.index_low-1:self.index_high]:\n",
    "                yield (\"index\", term)\n",
    "\n",
    "            # Yield all words\n",
    "            for term in TopTerms:\n",
    "                yield (\"words\", term)\n",
    "        else:\n",
    "            # Yield index words\n",
    "            for term in TopTerms[self.index_low-1:self.index_high]:\n",
    "                yield (\"index\", term[0])\n",
    "\n",
    "            # Yield all words\n",
    "            for term in TopTerms:\n",
    "                yield (\"words\", term[0])\n",
    "    \n",
    "    def steps(self):\n",
    "        \"\"\"\n",
    "        Step one: Yield top n-words from each reducer. Means dataset size is \n",
    "                  n-words * num_reducers. Guarantees overall top n-words are \n",
    "                  sent to the next step.\n",
    "        \n",
    "        \"\"\"\n",
    "        mr_steps = [MRStep(mapper_init=self.mapper_init,\n",
    "                           mapper=self.mapper,\n",
    "                           combiner=self.combiner,\n",
    "                           reducer_init=self.reducer_init,\n",
    "                           reducer_final=self.reducer_final,\n",
    "                           reducer=self.reducer),\n",
    "                    MRStep(mapper=self.mapper_single_key,\n",
    "                           reducer_init=self.reducer_init_top_vals,\n",
    "                           reducer=self.reducer_top_vals,\n",
    "                           reducer_final=self.reducer_final_top_vals)]\n",
    "        return mr_steps\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    GetIndexandOtherWords.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test getting the index and other valid words excluding stop words on the mini_5gram.txt dataset. Return the top 10 most common words (based on the term counts) and mark the ninth and tenth most common words as the index words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"index\"\t\"narrative\"\r\n",
      "\"index\"\t\"sea\"\r\n",
      "\"index\"\t\"establishing\"\r\n",
      "\"index\"\t\"religious\"\r\n",
      "\"index\"\t\"limited\"\r\n",
      "\"words\"\t\"child\"\r\n",
      "\"words\"\t\"christmas\"\r\n",
      "\"words\"\t\"wales\"\r\n",
      "\"words\"\t\"case\"\r\n",
      "\"words\"\t\"study\"\r\n",
      "\"words\"\t\"female\"\r\n",
      "\"words\"\t\"collection\"\r\n",
      "\"words\"\t\"fairy\"\r\n",
      "\"words\"\t\"tales\"\r\n",
      "\"words\"\t\"forms\"\r\n",
      "\"words\"\t\"government\"\r\n",
      "\"words\"\t\"general\"\r\n",
      "\"words\"\t\"george\"\r\n",
      "\"words\"\t\"circumstantial\"\r\n",
      "\"words\"\t\"city\"\r\n",
      "\"words\"\t\"narrative\"\r\n",
      "\"words\"\t\"sea\"\r\n",
      "\"words\"\t\"establishing\"\r\n",
      "\"words\"\t\"religious\"\r\n",
      "\"words\"\t\"limited\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat mini_5gram.txt | python GetIndexandOtherWords.py --index-range 16-20    \\\n",
    "                                                      --top-n-words 20       \\\n",
    "                                                      --return-counts False  \\\n",
    "                                                      --use-term-counts True \\\n",
    "                                                       -q > vocabs\n",
    "!cat vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To spot check the results, view the term counts of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"index\"\t[\"narrative\", 62]\r\n",
      "\"index\"\t[\"sea\", 62]\r\n",
      "\"index\"\t[\"establishing\", 59]\r\n",
      "\"index\"\t[\"religious\", 59]\r\n",
      "\"index\"\t[\"limited\", 55]\r\n",
      "\"words\"\t[\"child\", 1099]\r\n",
      "\"words\"\t[\"christmas\", 1099]\r\n",
      "\"words\"\t[\"wales\", 1099]\r\n",
      "\"words\"\t[\"case\", 604]\r\n",
      "\"words\"\t[\"study\", 604]\r\n",
      "\"words\"\t[\"female\", 447]\r\n",
      "\"words\"\t[\"collection\", 239]\r\n",
      "\"words\"\t[\"fairy\", 123]\r\n",
      "\"words\"\t[\"tales\", 123]\r\n",
      "\"words\"\t[\"forms\", 116]\r\n",
      "\"words\"\t[\"government\", 102]\r\n",
      "\"words\"\t[\"general\", 92]\r\n",
      "\"words\"\t[\"george\", 92]\r\n",
      "\"words\"\t[\"circumstantial\", 62]\r\n",
      "\"words\"\t[\"city\", 62]\r\n",
      "\"words\"\t[\"narrative\", 62]\r\n",
      "\"words\"\t[\"sea\", 62]\r\n",
      "\"words\"\t[\"establishing\", 59]\r\n",
      "\"words\"\t[\"religious\", 59]\r\n",
      "\"words\"\t[\"limited\", 55]\r\n"
     ]
    }
   ],
   "source": [
    "!cat mini_5gram.txt | python GetIndexandOtherWords.py --index-range 16-20    \\\n",
    "                                                      --top-n-words 20       \\\n",
    "                                                      --return-counts True   \\\n",
    "                                                      --use-term-counts True \\\n",
    "                                                       -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  HW5.3.2  Full-scale experiment: EDA of Google n-grams dataset (PHASE 2)\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We included all of this analysis at the end.\n",
    "\n",
    "See [here](#2.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.2\">\n",
    "## 2.2 Stripe Creation\n",
    "[Back to Table of Contents](#TOC)  \n",
    "\n",
    "The section takes the output from 2.1 and create stripes based on the vocabularies identified.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MakeStripes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MakeStripes.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from collections import Counter\n",
    "from sys import stderr\n",
    "from re import findall\n",
    "\n",
    "class MakeStripes(MRJob):\n",
    "    def mapper_init(self):\n",
    "        \"\"\"\n",
    "        Read in index words and word list.\n",
    "        \"\"\"\n",
    "        self.stripes = {}\n",
    "        \n",
    "        self.indexlist, self.wordslist = [],[]\n",
    "        with open('vocabs', 'r') as vocabFile:\n",
    "            for line in vocabFile:\n",
    "                word_type, word = line.replace('\"', '').split()\n",
    "                if word_type == 'index':\n",
    "                    self.indexlist.append(word)\n",
    "                else:\n",
    "                    self.wordslist.append(word)\n",
    "        \n",
    "        # Convert to sets to make lookups faster\n",
    "        self.indexlist = set(self.indexlist)\n",
    "        self.wordslist = set(self.wordslist)\n",
    "    \n",
    "    def mapper(self, _, lines):\n",
    "        \"\"\"\n",
    "        Make stripes using index and words list\n",
    "        \"\"\"\n",
    "        terms, term_count, page_count, book_count = lines.split(\"\\t\")\n",
    "        term_count = int(term_count)\n",
    "        terms = findall(r'[a-z]+', terms.lower())\n",
    "                \n",
    "        for item in terms:\n",
    "            if item in self.indexlist:\n",
    "                for val in terms:\n",
    "                    if val != item and val in self.wordslist:\n",
    "                        yield item, {val:term_count}\n",
    "        \n",
    "    def combiner(self, keys, values):\n",
    "        values_sum = Counter()\n",
    "        for val in values:\n",
    "            values_sum += Counter(val)\n",
    "        yield keys, dict(values_sum)\n",
    "\n",
    "    def reducer(self, keys, values):\n",
    "        values_sum = Counter()\n",
    "        for val in values:\n",
    "            values_sum += Counter(val)\n",
    "        yield keys, dict(values_sum)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MakeStripes.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"establishing\"\t{\"religious\": 59}\r\n",
      "\"limited\"\t{\"case\": 55, \"study\": 55}\r\n",
      "\"narrative\"\t{\"circumstantial\": 62}\r\n",
      "\"religious\"\t{\"establishing\": 59}\r\n",
      "\"sea\"\t{\"city\": 62}\r\n"
     ]
    }
   ],
   "source": [
    "!python MakeStripes.py --file vocabs mini_5gram.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.3\">\n",
    "## 2.3 Invert Index Creation\n",
    "[Back to Table of Contents](#TOC)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting InvertIndex.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile InvertIndex.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONProtocol\n",
    "from collections import Counter\n",
    "\n",
    "class InvertIndex(MRJob):\n",
    "    MRJob.input_protocol = JSONProtocol\n",
    "    \n",
    "    def mapper(self, key, words):\n",
    "        \"\"\"\n",
    "        Convert each stripe to inverted index\n",
    "        \"\"\"\n",
    "        n_words = len(words)\n",
    "        for word in words: \n",
    "            yield (word, {key:n_words})\n",
    "            \n",
    "    def combiner(self, keys, values):\n",
    "        values_sum = Counter()\n",
    "        for val in values:\n",
    "            values_sum += Counter(val)\n",
    "        yield keys, dict(values_sum)\n",
    "\n",
    "    def reducer(self, keys, values):\n",
    "        values_sum = Counter()\n",
    "        for val in values:\n",
    "            values_sum += Counter(val)\n",
    "        yield keys, dict(values_sum)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    InvertIndex.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"case\"\t{\"limited\": 2}\r\n",
      "\"circumstantial\"\t{\"narrative\": 1}\r\n",
      "\"city\"\t{\"sea\": 1}\r\n",
      "\"establishing\"\t{\"religious\": 1}\r\n",
      "\"religious\"\t{\"establishing\": 1}\r\n",
      "\"study\"\t{\"limited\": 2}\r\n"
     ]
    }
   ],
   "source": [
    "!python MakeStripes.py --file vocabs mini_5gram.txt -q | python InvertIndex.py -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.4\">\n",
    "## 2.4 Similarity Calculation\n",
    "[Back to Table of Contents](#TOC)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Similarity.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Similarity.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONProtocol\n",
    "from itertools import combinations\n",
    "\n",
    "class Similarity(MRJob):\n",
    "    MRJob.input_protocol = JSONProtocol\n",
    "    \n",
    "    def mapper(self, key_term, docs):\n",
    "        \"\"\"\n",
    "        Make co-occurrence keys for each pair of documents in the inverted\n",
    "        index and make keys representing each document.\n",
    "        \"\"\"\n",
    "        doc_names = docs.keys()\n",
    "        for doc_pairs in combinations(sorted(list(doc_names)), 2):\n",
    "            yield (doc_pairs, 1)\n",
    "        for name in doc_names:\n",
    "            yield (name, 1)\n",
    "            \n",
    "    def combiner(self, key, value):\n",
    "        yield (key, sum(value))\n",
    "        \n",
    "        \n",
    "    ### Custom partitioner code goes here\n",
    "    def reducer_init(self):\n",
    "        self.words = {}\n",
    "        self.results = []\n",
    "    \n",
    "    def reducer(self, doc_or_docs, count):\n",
    "        if isinstance(doc_or_docs, str):\n",
    "            self.words[doc_or_docs] = sum(count)\n",
    "        else:\n",
    "            d1, d2 = doc_or_docs\n",
    "            d1_n_words, d2_n_words = self.words[d1], self.words[d2]\n",
    "            intersection = float(sum(count))\n",
    "            \n",
    "            jaccard = round(intersection/(d1_n_words + d2_n_words - intersection), 3)\n",
    "            cosine = round(intersection/(d1_n_words**.5 * d2_n_words**.5), 3)\n",
    "            dice = round(2*intersection/(d1_n_words + d2_n_words), 3)\n",
    "            overlap = round(intersection/min(d1_n_words, d2_n_words), 3)\n",
    "            average = round(sum([jaccard, cosine, dice, overlap])/4.0, 3)\n",
    "            \n",
    "            self.results.append([doc_or_docs, {\"jacc\":jaccard, \"cos\":cosine, \n",
    "                                               \"dice\":dice, \"ol\":overlap, \"ave\":average}])\n",
    "            \n",
    "    def reducer_final(self):\n",
    "        for doc, result in sorted(self.results, key=lambda x: x[1][\"ave\"], reverse=True):\n",
    "            yield (doc, result)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    Similarity.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't return anything because there are no co-occurring words in the inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python MakeStripes.py --file vocabs mini_5gram.txt -q | python InvertIndex.py -q | python Similarity.py -q --jobconf mapred.reduce.tasks=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time the full calculations a slightly larger dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see how large the full dataset is. This number won't be exactly correct because my download was interrupted halfway through and I only have 185 items total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 57432975 459463800 2110162633\n",
      "CPU times: user 126 ms, sys: 37.3 ms, total: 163 ms\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!cat Temp_data/googlebooks-eng-all-5gram-20090715-* | wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are 200 files that make up the 5-gram dataset (at least that is what I thought I heard), the true line count of the dataset is about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62089702"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(57432975*(200/185))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to operate on a subset of this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3435179 27481432 126216026\n",
      "CPU times: user 9.55 ms, sys: 8.13 ms, total: 17.7 ms\n",
      "Wall time: 789 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!cat Temp_data/googlebooks-eng-all-5gram-20090715-9* | wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample of the data is only a few percent (about 5.5%) of the full size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05532606679284755"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3435179/62089702"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create index and words to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that many of the index words only occur one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"index\"\t[\"bereavements\", 1]\n",
      "\"index\"\t[\"augmente\", 1]\n",
      "\"index\"\t[\"berenice\", 1]\n",
      "\"index\"\t[\"berenson\", 1]\n",
      "\"index\"\t[\"beret\", 1]\n",
      "\"index\"\t[\"augmentis\", 1]\n",
      "\"index\"\t[\"bergey\", 1]\n",
      "\"index\"\t[\"berggren\", 1]\n",
      "\"index\"\t[\"bergs\", 1]\n",
      "\"index\"\t[\"berkel\", 1]\n",
      "CPU times: user 1.32 s, sys: 357 ms, total: 1.68 s\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!cat Temp_data/googlebooks-eng-all-5gram-20090715-9* | python GetIndexandOtherWords.py --index-range 9001-10000  \\\n",
    "                                                                                       --top-n-words 10000       \\\n",
    "                                                                                       --return-counts True     \\\n",
    "                                                                                       --use-term-counts False    \\\n",
    "                                                                                        -q > vocabs\n",
    "!head vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will return the term count of each word (not the 5gram-based count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"index\"\t[\"averil\", 63]\n",
      "\"index\"\t[\"bioreactor\", 63]\n",
      "\"index\"\t[\"bjork\", 63]\n",
      "\"index\"\t[\"blankest\", 63]\n",
      "\"index\"\t[\"blatea\", 63]\n",
      "\"index\"\t[\"blazon\", 63]\n",
      "\"index\"\t[\"badajos\", 63]\n",
      "\"index\"\t[\"ballantynes\", 63]\n",
      "\"index\"\t[\"bantered\", 63]\n",
      "\"index\"\t[\"brandenburgh\", 63]\n",
      "CPU times: user 1.52 s, sys: 399 ms, total: 1.92 s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!cat Temp_data/googlebooks-eng-all-5gram-20090715-9* | python GetIndexandOtherWords.py --index-range 9001-10000  \\\n",
    "                                                                                       --top-n-words 10000       \\\n",
    "                                                                                       --return-counts True     \\\n",
    "                                                                                       --use-term-counts True    \\\n",
    "                                                                                        -q > vocabs\n",
    "!head vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is very similar to what we would run on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"index\"\t\"averil\"\n",
      "\"index\"\t\"bioreactor\"\n",
      "\"index\"\t\"bjork\"\n",
      "\"index\"\t\"blankest\"\n",
      "\"index\"\t\"blatea\"\n",
      "\"index\"\t\"blazon\"\n",
      "\"index\"\t\"badajos\"\n",
      "\"index\"\t\"ballantynes\"\n",
      "\"index\"\t\"bantered\"\n",
      "\"index\"\t\"brandenburgh\"\n",
      "CPU times: user 1.29 s, sys: 353 ms, total: 1.64 s\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!cat Temp_data/googlebooks-eng-all-5gram-20090715-9* | python GetIndexandOtherWords.py --index-range 9001-10000  \\\n",
    "                                                                                       --top-n-words 10000       \\\n",
    "                                                                                       --return-counts False     \\\n",
    "                                                                                       --use-term-counts True    \\\n",
    "                                                                                        -q > vocabs\n",
    "!head vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make stripes, invert index, and calculate similarities. Print top similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"aufalle\", \"aufbau\"]\t{\"cos\": 1.0, \"ol\": 1.0, \"jacc\": 1.0, \"ave\": 1.0, \"dice\": 1.0}\n",
      "[\"aufalle\", \"aufgaben\"]\t{\"cos\": 1.0, \"ol\": 1.0, \"jacc\": 1.0, \"ave\": 1.0, \"dice\": 1.0}\n",
      "[\"aufalle\", \"auftrag\"]\t{\"cos\": 1.0, \"ol\": 1.0, \"jacc\": 1.0, \"ave\": 1.0, \"dice\": 1.0}\n",
      "[\"aufalle\", \"auftrage\"]\t{\"cos\": 1.0, \"ol\": 1.0, \"jacc\": 1.0, \"ave\": 1.0, \"dice\": 1.0}\n",
      "[\"aufalle\", \"ausgrabungen\"]\t{\"cos\": 1.0, \"ol\": 1.0, \"jacc\": 1.0, \"ave\": 1.0, \"dice\": 1.0}\n",
      "[\"aufalle\", \"baues\"]\t{\"cos\": 1.0, \"ol\": 1.0, \"jacc\": 1.0, \"ave\": 1.0, \"dice\": 1.0}\n",
      "[\"aufalle\", \"bayerischen\"]\t{\"cos\": 1.0, \"ol\": 1.0, \"jacc\": 1.0, \"ave\": 1.0, \"dice\": 1.0}\n",
      "[\"aufalle\", \"bearbeitung\"]\t{\"cos\": 1.0, \"ol\": 1.0, \"jacc\": 1.0, \"ave\": 1.0, \"dice\": 1.0}\n",
      "[\"aufalle\", \"bildende\"]\t{\"cos\": 1.0, \"ol\": 1.0, \"jacc\": 1.0, \"ave\": 1.0, \"dice\": 1.0}\n",
      "[\"aufbau\", \"aufgaben\"]\t{\"cos\": 1.0, \"ol\": 1.0, \"jacc\": 1.0, \"ave\": 1.0, \"dice\": 1.0}\n",
      "CPU times: user 219 ms, sys: 66.3 ms, total: 285 ms\n",
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!cat Temp_data/googlebooks-eng-all-5gram-20090715-9* | python MakeStripes.py --file vocabs -q | python InvertIndex.py -q | python Similarity.py -q --jobconf mapred.reduce.tasks=1 > similarities.txt\n",
    "!head similarities.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It about 3 minutes to run this code. The code processes 11 out of 200 files. It currently uses one machine. If the cluster has 50 machines available, we would expect it to take only a few minutes for these core operations to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0909090909090908"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minutes_for_small_job = 3\n",
    "n_small_jobs_in_big_job = 200/11\n",
    "total_minutes_one_computer = minutes_for_small_job*n_small_jobs_in_big_job\n",
    "computers_in_cluster = 50\n",
    "total_minutes_for_cluster = total_minutes_one_computer/computers_in_cluster\n",
    "total_minutes_for_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental code on custom partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import ascii_letters\n",
    "ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CustomPartitioner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile CustomPartitioner.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from sys import stderr\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from random import random\n",
    "\n",
    "\n",
    "class CustomPartitioner(MRJob):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomPartitioner, self).__init__(*args, **kwargs)\n",
    "        self.N = 30\n",
    "        self.NUM_REDUCERS = 2\n",
    "\n",
    "    def mapper_init(self):\n",
    "\n",
    "        def makeKeyHash(key, num_reducers):\n",
    "            byteof = lambda char: int(format(ord(char), 'b'), 2)\n",
    "            current_hash = 0\n",
    "            for c in key:\n",
    "                current_hash = (current_hash * 31 + byteof(c))\n",
    "            return current_hash % num_reducers\n",
    "\n",
    "        # printable ascii characters, starting with 'A'\n",
    "        keys = [str(chr(i)) for i in range(65,65+self.NUM_REDUCERS)]\n",
    "        partitions = []\n",
    "\n",
    "        for key in keys:\n",
    "            partitions.append([key, makeKeyHash(key, self.NUM_REDUCERS)])\n",
    "\n",
    "        parts = sorted(partitions,key=itemgetter(1))\n",
    "        self.partition_keys = list(np.array(parts)[:,0])\n",
    "\n",
    "        self.partition_file = np.arange(0,self.N,self.N/(self.NUM_REDUCERS))[::-1]\n",
    "        \n",
    "        print((keys, partitions, parts, self.partition_keys, self.partition_file), file=stderr)\n",
    "\n",
    "        \n",
    "    def mapper(self, _, lines):\n",
    "        terms, term_count, page_count, book_count = lines.split(\"\\t\")\n",
    "        terms = terms.split()\n",
    "        term_count = int(term_count)\n",
    "        \n",
    "        for item in terms:\n",
    "            yield (item, term_count)\n",
    "            \n",
    "        for item in [\"A\", \"B\", \"H\", \"I\"]:\n",
    "            yield (item, 0)\n",
    "            \n",
    "    def reducer_init(self):\n",
    "        self.reducer_unique_key = round(random(), 5)\n",
    "\n",
    "    \n",
    "    def reducer(self, keys, values):\n",
    "        yield (self.reducer_unique_key, (keys, sum(values)))\n",
    "        \n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    CustomPartitioner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/CustomPartitioner.Jason.20161013.010909.519687\n",
      "Running step 1 of 1...\n",
      "reading from STDIN\n",
      "Streaming final output from /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/CustomPartitioner.Jason.20161013.010909.519687/output...\n",
      "0.22079\t[\"A\", 0]\n",
      "0.22079\t[\"B\", 0]\n",
      "0.22079\t[\"H\", 0]\n",
      "0.22079\t[\"I\", 0]\n",
      "0.22079\t[\"atlas\", 65]\n",
      "0.45398\t[\"boon\", 60]\n",
      "0.45398\t[\"cava\", 10]\n",
      "0.45398\t[\"dipped\", 25]\n",
      "Removing temp directory /var/folders/sz/4k2bbjts7x5fmg9sn7kh6hlw0000gn/T/CustomPartitioner.Jason.20161013.010909.519687...\n"
     ]
    }
   ],
   "source": [
    "!cat atlas.txt | python CustomPartitioner.py -r local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CustomPartitioner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile CustomPartitioner.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class CustomPartitioner(MRJob):\n",
    "    def __init__(self, *args, <strong>kwargs):\n",
    "        super(CustomPartitioner, self).__init__(*args, <strong>kwargs)\n",
    "        self.N = 30\n",
    "        self.NUM_REDUCERS = 3\n",
    "\n",
    "    def mapper_partitioner_init(self):\n",
    "\n",
    "        def makeKeyHash(key, num_reducers):\n",
    "            byteof = lambda char: int(format(ord(char), 'b'), 2)\n",
    "            current_hash = 0\n",
    "            for c in key:\n",
    "                current_hash = (current_hash * 31 + byteof(c))\n",
    "            return current_hash % num_reducers\n",
    "\n",
    "        # printable ascii characters, starting with 'A'\n",
    "        keys = [str(unichr(i)) for i in range(65,65+self.NUM_REDUCERS)]\n",
    "        partitions = []\n",
    "\n",
    "        for key in keys:\n",
    "            partitions.append([key, makeKeyHash(key, self.NUM_REDUCERS)])\n",
    "\n",
    "        parts = sorted(partitions,key=itemgetter(1))\n",
    "        self.partition_keys = list(np.array(parts)[:,0])\n",
    "\n",
    "        self.partition_file = np.arange(0,self.N,self.N/(self.NUM_REDUCERS))[::-1]\n",
    "        \n",
    "    def mapper(self, _, lines):\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    CustomPartitioner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.5\">\n",
    "## 2.5 Ngram Ranking and Plotting \n",
    "[Back to Table of Contents](#TOC)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ngram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ngram.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "class NGram(MRJob):\n",
    "    def mapper_init(self):\n",
    "        self.length = 0\n",
    "        self.longest = 0\n",
    "        self.distribution = Counter()\n",
    "\n",
    "    def mapper(self, _, lines):\n",
    "        # extract word/count sets\n",
    "        ngram, count, pages, _ = lines.split(\"\\t\")\n",
    "        count, pages = int(count), int(pages)\n",
    "        \n",
    "        # loop to count word length\n",
    "        words = ngram.lower().split()\n",
    "        for w in words:\n",
    "            yield (w, {'count':count, 'pages':pages})\n",
    "        \n",
    "        # Count of ngram length\n",
    "        n_gram_character_count = len(ngram)\n",
    "        yield n_gram_character_count, count\n",
    "        \n",
    "        # determine if longest word on mapper\n",
    "        if n_gram_character_count > self.length:\n",
    "            self.length = n_gram_character_count\n",
    "            self.longest = [words, n_gram_character_count]\n",
    "            yield (self.longest)\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        if isinstance(word,str):\n",
    "            count = 0\n",
    "            pages = 0\n",
    "            for x in counts:\n",
    "                count += x['count']\n",
    "                pages += x['pages']\n",
    "            yield word, {'count':count,'pages':pages}\n",
    "        \n",
    "        #aggregate counts\n",
    "        elif isinstance(word,int):\n",
    "            yield word, sum(counts)\n",
    "        \n",
    "        #yield long ngrams\n",
    "        else:\n",
    "            for x in counts:\n",
    "                yield word, x\n",
    "                  \n",
    "    def reducer_init(self):\n",
    "        self.longest = []\n",
    "        self.length = 0\n",
    "        self.counts = Counter()\n",
    "        self.pages = Counter()\n",
    "        self.distribution = Counter()\n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        # use Counter word totals\n",
    "        for val in values:\n",
    "            if isinstance(key,str):\n",
    "                self.counts += Counter({key:val['count']})\n",
    "                self.pages += Counter({key:val['pages']})\n",
    "        \n",
    "        # aggregate distribution numbers\n",
    "            elif isinstance(key,int):\n",
    "                self.distribution += Counter({key:val})\n",
    "            else:\n",
    "                # Determine if longest ngram on reducer\n",
    "                if val > self.length:\n",
    "                        self.longest = [key, val]\n",
    "                        self.length = val\n",
    "\n",
    "    def reducer_final(self):\n",
    "        # yield density calculation\n",
    "        for x in sorted(self.counts):\n",
    "            yield ('mrj_dens',{x:(1.*self.counts[x]/self.pages[x])})\n",
    "        \n",
    "        # Use most_common counter function\n",
    "        for x in self.counts.most_common(10):\n",
    "            yield x\n",
    "\n",
    "        # return longest item\n",
    "        if self.longest:\n",
    "            yield self.longest\n",
    "        \n",
    "        # yield distribution values\n",
    "        for x in self.distribution:\n",
    "            yield ('mrj_dist', {x:self.distribution[x]})\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    NGram.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"mrj_dens\"\t{\"a\": 1.0282931354359925}\r\n",
      "\"mrj_dens\"\t{\"bill\": 1.0}\r\n",
      "\"mrj_dens\"\t{\"biography\": 1.0222222222222221}\r\n",
      "\"mrj_dens\"\t{\"by\": 1.0333333333333334}\r\n",
      "\"mrj_dens\"\t{\"case\": 1.0}\r\n",
      "\"mrj_dens\"\t{\"child's\": 1.0358152686145146}\r\n",
      "\"mrj_dens\"\t{\"christmas\": 1.0358152686145146}\r\n",
      "\"mrj_dens\"\t{\"circumstantial\": 1.0}\r\n",
      "\"mrj_dens\"\t{\"city\": 1.0333333333333334}\r\n",
      "\"mrj_dens\"\t{\"collection\": 1.0863636363636364}\r\n",
      "\"mrj_dens\"\t{\"establishing\": 1.0}\r\n",
      "\"mrj_dens\"\t{\"fairy\": 1.0512820512820513}\r\n",
      "\"mrj_dens\"\t{\"female\": 1.0}\r\n",
      "\"mrj_dens\"\t{\"for\": 1.0}\r\n",
      "\"mrj_dens\"\t{\"forms\": 1.1262135922330097}\r\n",
      "\"mrj_dens\"\t{\"general\": 1.0222222222222221}\r\n",
      "\"mrj_dens\"\t{\"george\": 1.0222222222222221}\r\n",
      "\"mrj_dens\"\t{\"government\": 1.0}\r\n",
      "\"mrj_dens\"\t{\"in\": 1.0326741186586414}\r\n",
      "\"mrj_dens\"\t{\"limited\": 1.0}\r\n",
      "\"mrj_dens\"\t{\"narrative\": 1.0}\r\n",
      "\"mrj_dens\"\t{\"of\": 1.0348004094165815}\r\n",
      "\"mrj_dens\"\t{\"religious\": 1.0}\r\n",
      "\"mrj_dens\"\t{\"sea\": 1.0333333333333334}\r\n",
      "\"mrj_dens\"\t{\"study\": 1.0}\r\n",
      "\"mrj_dens\"\t{\"tales\": 1.0512820512820513}\r\n",
      "\"mrj_dens\"\t{\"the\": 1.0163934426229508}\r\n",
      "\"mrj_dens\"\t{\"wales\": 1.0358152686145146}\r\n",
      "\"a\"\t2217\r\n",
      "\"in\"\t1201\r\n",
      "\"child's\"\t1099\r\n",
      "\"wales\"\t1099\r\n",
      "\"christmas\"\t1099\r\n",
      "\"of\"\t1011\r\n",
      "\"case\"\t604\r\n",
      "\"study\"\t604\r\n",
      "\"female\"\t447\r\n",
      "\"collection\"\t239\r\n",
      "[\"a\", \"bill\", \"for\", \"establishing\", \"religious\"]\t33\r\n",
      "\"mrj_dist\"\t{\"33\": 121}\r\n",
      "\"mrj_dist\"\t{\"17\": 62}\r\n",
      "\"mrj_dist\"\t{\"22\": 447}\r\n",
      "\"mrj_dist\"\t{\"23\": 55}\r\n",
      "\"mrj_dist\"\t{\"24\": 116}\r\n",
      "\"mrj_dist\"\t{\"26\": 102}\r\n",
      "\"mrj_dist\"\t{\"27\": 123}\r\n",
      "\"mrj_dist\"\t{\"28\": 1099}\r\n",
      "\"mrj_dist\"\t{\"29\": 92}\r\n"
     ]
    }
   ],
   "source": [
    "!python ngram.py --jobconf mapred.reduce.tasks=1 < googlebooks-eng-all-5gram-20090715-0-filtered-first-10-lines.txt -q > dataout.txt\n",
    "!cat dataout.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('forms', 1.1262135922330097), ('collection', 1.0863636363636364), ('tales', 1.0512820512820513), ('fairy', 1.0512820512820513), ('wales', 1.0358152686145146), ('christmas', 1.0358152686145146), (\"child's\", 1.0358152686145146), ('of', 1.0348004094165815), ('sea', 1.0333333333333334), ('city', 1.0333333333333334), ('by', 1.0333333333333334), ('in', 1.0326741186586414), ('a', 1.0282931354359925), ('general', 1.0222222222222221), ('george', 1.0222222222222221), ('biography', 1.0222222222222221), ('the', 1.0163934426229508), ('limited', 1.0), ('female', 1.0), ('religious', 1.0)]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f33fc160950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sorted density list\n",
    "def density(data):\n",
    "    x = data\n",
    "    sorted_x = sorted(x.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    print sorted_x[:20]\n",
    "\n",
    "# distribution plot\n",
    "def distribution(data):\n",
    "    plt.scatter(data.keys(), data.values(), alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "# loader\n",
    "def driver():\n",
    "    datain = open('dataout.txt','r')\n",
    "\n",
    "    densdata = {}\n",
    "    distdata = {}\n",
    "\n",
    "    # clean the mess I made\n",
    "    for line in datain:\n",
    "        parts = line.split('\\t')\n",
    "        temp = parts[1][1:-2].replace('\"', '').split(':')\n",
    "        mrj_val = parts[0].replace('\"', '')\n",
    "        if mrj_val == \"mrj_dens\":\n",
    "            densdata[temp[0]]=float(temp[1])\n",
    "        elif mrj_val == \"mrj_dist\":\n",
    "            distdata[int(temp[0])]=int(temp[1])\n",
    "    \n",
    "    #Execute density sort\n",
    "    density(densdata)\n",
    "    \n",
    "    #Execute distribution plot\n",
    "    distribution(distdata)\n",
    "            \n",
    "driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.6\">\n",
    "## 2.6 NLTK Benchmarking\n",
    "[Back to Table of Contents](#TOC) \n",
    "\n",
    "This section examine the output pairs using nltk library.  \n",
    "For each pair of words, we examine whether one is identified as a synonym of the other by nltk.  \n",
    "Based on the \"hit\" data, we compute precision, recall and F1 score of the output.  \n",
    "\n",
    "**With limited pair in the output, it is possible to run everything within a python script.  \n",
    "We also prepare a mapreduce job in case the number of pairs increase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NLTKBenchMark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NLTKBenchMark.py\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class NLTKBenchMark(MRJob):\n",
    "    \n",
    "    def mapper(self, _, lines):\n",
    "        #parse the output file and identify the pair of words\n",
    "        pair, avg = lines.split(\"\\t\")\n",
    "        pair = json.loads(pair)\n",
    "        word1, word2 = pair[0], pair[1]        \n",
    "        \n",
    "        hit = 0\n",
    "        \n",
    "        #for each word, extract the list of synonyms from nltk corpus, convert to set to remove duplicates\n",
    "        syn1 = set([l.name() for s in wn.synsets(word1) for l in s.lemmas()])\n",
    "        syn2 = set([l.name() for s in wn.synsets(word2) for l in s.lemmas()])\n",
    "        \n",
    "        #keep track of words that have no synonym using '~nosync'\n",
    "        if len(syn1) == 0:\n",
    "            yield '~nosyn', [word1]\n",
    "        if len(syn2) == 0:\n",
    "            yield '~nosyn', [word2]\n",
    "            \n",
    "        '''\n",
    "        for each occurence of word, increment the count\n",
    "        for word A, synset is the number of synonyms of the other word B\n",
    "        this value is used for calculating recall\n",
    "        this method becomes confusing/problematic if a word appears multiple times in the final output\n",
    "        \n",
    "        if there is a hit for word A, set the hit to 1, and set the hit for the other word B to 0 (to avoid double count)\n",
    "        if there is not a hit for A and B, set the hit to 0 for both\n",
    "        '''\n",
    "        if word2 in syn1:\n",
    "            yield word2, {'hit':1, 'count':1, 'synset':len(syn1)}\n",
    "            yield word1, {'hit':0, 'count':1, 'synset':len(syn2)}\n",
    "        elif word1 in syn2:\n",
    "            yield word1, {'hit':1, 'count':1, 'synset':len(syn2)}\n",
    "            yield word2, {'hit':0, 'count':1, 'synset':len(syn1)}\n",
    "        else:\n",
    "            yield word1, {'hit':0, 'count':1, 'synset':len(syn2)}\n",
    "            yield word2, {'hit':0, 'count':1, 'synset':len(syn1)}\n",
    "        \n",
    "    def combiner(self, term, values):\n",
    "        #combine '~nosyn' into a bigger list and yield the list\n",
    "        if term == '~nosyn':\n",
    "            nosynList = []\n",
    "            for value in values:\n",
    "                nosynList = nosynList+value\n",
    "            yield term, nosynList\n",
    "            \n",
    "        else:\n",
    "            counters = {'hit':0, 'count':0, 'synset':0}\n",
    "            for value in values:\n",
    "                counters['hit'] += value['hit']\n",
    "                counters['count'] += value['count']\n",
    "                counters['synset'] = value['synset']\n",
    "            yield term, counters\n",
    "        \n",
    "        \n",
    "    def reducer_init(self):\n",
    "        self.plist = []\n",
    "        self.rlist = []\n",
    "        self.flist = []\n",
    "        \n",
    "    def reducer(self, term, values):\n",
    "        #yield the final list of words that have no synonym\n",
    "        if term == '~nosyn':\n",
    "            nosynList = []\n",
    "            for value in values:\n",
    "                nosynList = nosynList+value\n",
    "            yield term, nosynList\n",
    "            \n",
    "        else:\n",
    "            counters = {'hit':0.0, 'count':0.0, 'synset':0.0}\n",
    "            precision, recall, F1 = 0,0,0\n",
    "            for value in values:\n",
    "                counters['hit'] += value['hit']\n",
    "                counters['count'] += value['count']\n",
    "                counters['synset'] = value['synset']\n",
    "                \n",
    "            if counters['hit'] > 0 and counters['synset'] > 0:\n",
    "                precision = float(counters['hit'])/float(counters['count'])\n",
    "                recall = float(counters['hit'])/float(counters['synset'])\n",
    "                F1 = 2*precision*recall/(precision+recall)\n",
    "                \n",
    "                self.plist.append(precision)\n",
    "                self.rlist.append(recall)\n",
    "                self.flist.append(F1)\n",
    "                yield term, counters\n",
    "            elif counters['synset'] > 0:\n",
    "                self.plist.append(precision)\n",
    "                self.rlist.append(recall)\n",
    "                self.flist.append(F1)\n",
    "                yield term, counters\n",
    "    \n",
    "    def reducer_final(self):\n",
    "        #compute the mean of all collected measurements\n",
    "        yield 'precision', np.mean(self.plist)\n",
    "        yield 'recall', np.mean(self.rlist)\n",
    "        yield 'F1', np.mean(self.flist)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    NLTKBenchMark.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/NLTKBenchMark.cloudera.20161011.115908.452455\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/NLTKBenchMark.cloudera.20161011.115908.452455/output...\n",
      "\"bad\"\t{\"count\": 2.0, \"synset\": 65, \"hit\": 0.0}\n",
      "\"candy\"\t{\"count\": 1.0, \"synset\": 8, \"hit\": 0.0}\n",
      "\"cat\"\t{\"count\": 1.0, \"synset\": 37, \"hit\": 0.0}\n",
      "\"cotton\"\t{\"count\": 1.0, \"synset\": 5, \"hit\": 0.0}\n",
      "\"dog\"\t{\"count\": 1.0, \"synset\": 46, \"hit\": 0.0}\n",
      "\"fairy\"\t{\"count\": 1.0, \"synset\": 9, \"hit\": 0.0}\n",
      "\"glad\"\t{\"count\": 2.0, \"synset\": 7, \"hit\": 0.0}\n",
      "\"good\"\t{\"count\": 2.0, \"synset\": 34, \"hit\": 0.0}\n",
      "\"precision\"\t0.0\n",
      "\"recall\"\t0.0\n",
      "\"F1\"\t0.0\n",
      "\"man\"\t{\"count\": 1.0, \"synset\": 11, \"hit\": 0.0}\n",
      "\"mario\"\t{\"count\": 1.0, \"synset\": 7, \"hit\": 0.0}\n",
      "\"pikachu\"\t{\"count\": 1.0, \"synset\": 57, \"hit\": 0.0}\n",
      "\"tale\"\t{\"count\": 1.0, \"synset\": 16, \"hit\": 0.0}\n",
      "\"unhappy\"\t{\"count\": 1.0, \"synset\": 10, \"hit\": 0.0}\n",
      "\"well\"\t{\"count\": 1.0, \"synset\": 65, \"hit\": 1.0}\n",
      "\"woman\"\t{\"count\": 1.0, \"synset\": 33, \"hit\": 0.0}\n",
      "\"~nosyn\"\t[\"mario\", \"pikachu\"]\n",
      "\"precision\"\t0.14285714285714285\n",
      "\"recall\"\t0.002197802197802198\n",
      "\"F1\"\t0.00432900432900433\n",
      "Removing temp directory /tmp/NLTKBenchMark.cloudera.20161011.115908.452455...\n"
     ]
    }
   ],
   "source": [
    "!python NLTKBenchMark.py nltk_bench_sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Number of Hits: 1 out of top 10\n",
      "Number of words without synonyms: 2\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Precision\t0.0666666666667\n",
      "Recall\t\t0.0018018018018\n",
      "F1\t\t0.00350877192982\n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Words without synonyms:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[] pikachu\n",
      "[] mario\n"
     ]
    }
   ],
   "source": [
    "''' Performance measures '''\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    \n",
    "    return syndict.keys()\n",
    "\n",
    "hits = []\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "\n",
    "TOTAL = 0\n",
    "flag = False # so we don't double count, but at the same time don't miss hits\n",
    "\n",
    "## For this part we can use one of three outputs. They are all the same, but were generated differently\n",
    "# 1. the top 1000 from the full sorted dataset -> sortedSims[:1000]\n",
    "# 2. the top 1000 from the partial sort aggragate file -> sims2/top1000sims\n",
    "# 3. the top 1000 from the total order sort file -> head -1000 sims_parts/part-00004\n",
    "\n",
    "top1000sims = []\n",
    "with open(\"nltk_bench_sample.txt\",\"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        lisst, avg = line.split(\"\\t\")\n",
    "        lisst = eval(lisst)\n",
    "        lisst.append(avg)\n",
    "        top1000sims.append(lisst)\n",
    "    \n",
    "\n",
    "measures = {}\n",
    "not_in_wordnet = []\n",
    "\n",
    "for line in top1000sims:\n",
    "    TOTAL += 1\n",
    "    words=line[0:2]\n",
    "    \n",
    "    \n",
    "    for word in words:\n",
    "        if word not in measures:\n",
    "            measures[word] = {\"syns\":0,\"opps\": 0,\"hits\":0}\n",
    "        measures[word][\"opps\"] += 1 \n",
    "    \n",
    "    syns0 = synonyms(words[0])\n",
    "    measures[words[1]][\"syns\"] = len(syns0)\n",
    "    if len(syns0) == 0:\n",
    "        not_in_wordnet.append(words[0])\n",
    "        \n",
    "    if words[1] in syns0:\n",
    "        TP += 1\n",
    "        hits.append(line)\n",
    "        flag = True\n",
    "        measures[words[1]][\"hits\"] += 1\n",
    "        \n",
    "    syns1 = synonyms(words[1]) \n",
    "    measures[words[0]][\"syns\"] = len(syns1)\n",
    "    if len(syns1) == 0:\n",
    "        not_in_wordnet.append(words[1])\n",
    "\n",
    "    if words[0] in syns1:\n",
    "        if flag == False:\n",
    "            TP += 1\n",
    "            hits.append(line)\n",
    "            measures[words[0]][\"hits\"] += 1\n",
    "            \n",
    "    flag = False    \n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for key in measures:\n",
    "    p,r,f = 0,0,0\n",
    "    if measures[key][\"hits\"] > 0 and measures[key][\"syns\"] > 0:\n",
    "        p = measures[key][\"hits\"]/measures[key][\"opps\"]\n",
    "        r = measures[key][\"hits\"]/measures[key][\"syns\"]\n",
    "        f = 2 * (p*r)/(p+r)\n",
    "    \n",
    "    # For calculating measures, only take into account words that have synonyms in wordnet\n",
    "    if measures[key][\"syns\"] > 0:\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1.append(f)\n",
    "\n",
    "    \n",
    "# Take the mean of each measure   \n",
    "print \"—\"*110    \n",
    "print \"Number of Hits:\",TP, \"out of top\",TOTAL\n",
    "print \"Number of words without synonyms:\",len(not_in_wordnet)\n",
    "print \"—\"*110 \n",
    "print \"Precision\\t\", np.mean(precision)\n",
    "print \"Recall\\t\\t\", np.mean(recall)\n",
    "print \"F1\\t\\t\", np.mean(f1)\n",
    "print \"—\"*110  \n",
    "\n",
    "print \"Words without synonyms:\"\n",
    "print \"-\"*100\n",
    "\n",
    "for word in not_in_wordnet:\n",
    "    print synonyms(word),word"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
